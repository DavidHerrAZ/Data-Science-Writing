{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing A Data Science Blog Post\n",
    "## \n",
    "\n",
    "### By: David Herr\n",
    "### Dated: August 2nd, 2020\n",
    "\n",
    "A Jupyter notebook aimed to complete the Udacity Data Science Nanodegree Project 1. In the project, students are to use the CRISP-DM process to understand, prepare, model, and evaluate data to answer real-world business questions.\n",
    "\n",
    "First, we'll bring in the packages used to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\dlher\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\dlher\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\dlher\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# data import and manipulation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting and graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# custom modules\n",
    "import fns_DataVisualization as viz\n",
    "import fns_DataWrangling as DataWrngl\n",
    "import fns_TextWrangling as TxtWrngl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Pick A Dataset\n",
    "\n",
    "Although the traditional CRISP-DM path is to start with business questions, the Udacity Data Science program provided datasets to start with. Thus, we'll start with those datasets and read them in to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Boston dataset\n",
    "df_boston = pd.read_csv('./_data/listings_boston.csv')\n",
    "\n",
    "# Read in Seattle dataset\n",
    "df_seattle = pd.read_csv('./_data/listings_seattle.csv')\n",
    "\n",
    "# Place dataframes in list for bulk analysis\n",
    "df_list = [df_boston,df_seattle]\n"
   ]
  },
  {
   "source": [
    "### 2) Pose 3-5 Business Questions\n",
    "Leveraging this data, we are curious to find out if:\n",
    "\n",
    "    1. Bedroom or Bathroom count generally correlates to higher pricing?\n",
    "    2. Are there certain neighborhoods which command generally higher pricing?\n",
    "    3. Does containing a top N keyword in descriptions result in higher pricing?\n",
    "    4. Are ratings predictive of pricing?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3) Use CRISP_DM \n",
    "To answer these questions, we'll use what is known as the CRISP_DM process. This process is pictated below:\n",
    "\n",
    "![Image of CRISP_DM](_img\\CRISP_DM.png)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "##### 3.1) Business Understanding\n",
    "\n",
    "**Boston Bed/Bath Analysis**\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns to be used for price vs. bed/bath analysis\n",
    "bed_bath_cols = ['price','bedrooms','bathrooms','beds','bed_type']\n",
    "\n",
    "# call 2x2 price analysis plot for all bed/bath cols (Boston)\n",
    "viz.price_analysis_plots(df_boston[bed_bath_cols])"
   ]
  },
  {
   "source": [
    "**Boston Neighborhood Analysis**\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns to be used for price vs. neighbourhood analysis\n",
    "neighborhood_cols = ['price','neighbourhood_cleansed']\n",
    "\n",
    "# call 1x1 price analysis plot for neighborhoods cols (Boston)\n",
    "viz.price_analysis_plots(df_boston[neighborhood_cols], False)"
   ]
  },
  {
   "source": [
    "##### 3.2) Data Understanding\n",
    "\n",
    "While the head() method is useful, a more thorough understanding of all the available columns, datatypes, as well as profile of that data, is necessary before moving on to preparing the data for modeling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# And see the overall shape of each data frame, column names, and column data types\n",
    "wrngl.print_column_types(df_list)"
   ]
  },
  {
   "source": [
    "##### 3.3) Data Preperation\n",
    "In this section, we will be using our above observations to guide both the initial cleansing of our data, as well as initial transformations for feature creation.\n",
    "\n",
    "**Column Cleansing**\n",
    "***\n",
    "\n",
    "We'll start with the data cleansing process on the columns. This will largely involve removing:\n",
    "\n",
    "- [x]  100% empty\n",
    "- [x]  Columns with 100% of the same value\n",
    "- [x]  Database identifications\n",
    "- [x]  Non-overlapping columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with dropping columns in all data sets that are 100% na\n",
    "DataWrngl.drop_na_columns(df_list)\n",
    "\n",
    "# Define search words to drop common identification column suffixes\n",
    "search_words = ['_id','_url','_scraped']\n",
    "\n",
    "# Invoke function to drop id, url, and scraping columns\n",
    "DataWrngl.drop_ident_columns(df_list,search_words)\n",
    "\n",
    "# Invoke function to drop any columns containing rows with 100% the same data point\n",
    "DataWrngl.drop_same_columns(df_list)\n",
    "\n",
    "# Invoke function to drop non-overlapping columns between two datasets    \n",
    "DataWrngl.drop_nonoverlap_columns(df_list[0],df_list[1])\n",
    "\n",
    "# Check to make sure all columns are overlapping between dataframes\n",
    "sum((df_boston.columns == df_seattle.columns)) / ((len(df_boston.columns) + len(df_seattle.columns)) / 2 ) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row & Data Cleansing**\n",
    "***\n",
    "Next, we'll move into basic *row cleansing* for items we know will break a linear regression model. This includes:\n",
    "\n",
    "- [x]  True/False - transform to a true boolean rather than text \"t\" or \"f\".\n",
    "- [x]  Substantial Text - must be transformed into a text analytics feature.\n",
    "- [ ]  Date Columns - drop or transform into a duration if business questions warrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Invoke function to change 't' or 'f' to true Boolean values\n",
    "DataWrngl.cleanse_data_tf_to_boolean(df_list)\n",
    "\n",
    "#  Define text columns which require processing for text analytics\n",
    "text_columns = ['name','summary','space','description','neighborhood_overview', 'notes']\n",
    "\n",
    "# Invoke tokenization as add columns in both dataframes\n",
    "TxtWrngl.tokenize_text_columns(df_list,text_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize count vectorizer object\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "\n",
    "# get counts of each token (word) in text data\n",
    "top_vocab = vectorizer.fit_transform(df_boston['summary'].apply(str))\n",
    "\n",
    "# Create dataframe to hold vocabulary and count of each item\n",
    "top_vocab_df = pd.DataFrame((count,word) for word, count in zip(sum(top_vocab.toarray()).tolist(),vectorizer.get_feature_names()))\n",
    "\n",
    "# Assign column names to new dataframe\n",
    "top_vocab_df.columns = ['Word', 'Count']\n",
    "\n",
    "# Sort descending to see top words\n",
    "top_vocab_df.sort_values('Count', ascending=False, inplace=True)\n",
    "\n",
    "# Preview results of dataframe\n",
    "top_vocab_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the top 20 words to a list for feature extraction\n",
    "top_vocab_df.nlargest(20,'Count')['Word'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4) Data Modeling\n",
    "\n",
    "[Outline placeholder for data modeling section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5) Results Evaluation\n",
    "\n",
    "[Outline placeholder for evaluation of model and possible re-modeling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6) Deployment\n",
    "\n",
    "[Outline placeholder for what deployment steps would be]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Conclusion\n",
    "\n",
    "[Outline placeholder for final remarks]"
   ]
  }
 ]
}