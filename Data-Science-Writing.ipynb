{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing A Data Science Blog Post\n",
    "## \n",
    "\n",
    "### By: David Herr\n",
    "### Dated: August 2nd, 2020\n",
    "\n",
    "A Jupyter notebook aimed to complete the Udacity Data Science Nanodegree Project 1. In the project, students are to use the CRISP-DM process to understand, prepare, model, and evaluate data to answer real-world business questions.\n",
    "\n",
    "First, we'll bring in the packages used to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import and manipulation \n",
    "import pandas as pd\n",
    "\n",
    "# plotting and graphing\n",
    "%matplotlib inline\n",
    "\n",
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# custom modules\n",
    "import fns_DataVisualization as viz\n",
    "import fns_DataWrangling as DataWrngl\n",
    "import fns_TextWrangling as TxtWrngl\n",
    "\n",
    "#temp library\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Pick A Dataset\n",
    "\n",
    "Although the traditional CRISP-DM path is to start with business questions, the Udacity Data Science program provided datasets to start with. Thus, we'll start with those datasets and read them in to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Boston dataset\n",
    "df_boston = pd.read_csv('./_data/listings_boston.csv')\n",
    "\n",
    "# Read in Seattle dataset\n",
    "df_seattle = pd.read_csv('./_data/listings_seattle.csv')\n",
    "\n",
    "# Place dataframes in list for bulk analysis\n",
    "df_list = [df_boston,df_seattle]\n"
   ]
  },
  {
   "source": [
    "### 2) Pose 3-5 Business Questions\n",
    "Leveraging this data, we are curious to find out if:\n",
    "\n",
    "    1. Bedroom or Bathroom count generally correlates to higher pricing?\n",
    "    2. Are there certain neighborhoods which command generally higher pricing?\n",
    "    3. Does containing a top N keyword in descriptions result in higher pricing?\n",
    "    4. Are ratings predictive of pricing?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3) Use CRISP_DM \n",
    "To answer these questions, we'll use what is known as the CRISP_DM process. This process is pictated below:\n",
    "\n",
    "![Image of CRISP_DM](_img\\CRISP_DM.png)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "##### 3.1) Business Understanding\n",
    "\n",
    "**Boston Bed/Bath Analysis**\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns to be used for price vs. bed/bath analysis\n",
    "bed_bath_cols = ['price','bedrooms','bathrooms','beds','bed_type']\n",
    "\n",
    "# call 2x2 price analysis plot for all bed/bath cols (Boston)\n",
    "viz.price_analysis_plots(df_boston[bed_bath_cols])"
   ]
  },
  {
   "source": [
    "**Boston Neighborhood Analysis**\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns to be used for price vs. neighbourhood analysis\n",
    "neighborhood_cols = ['price','neighbourhood_cleansed']\n",
    "\n",
    "# call 1x1 price analysis plot for neighborhoods cols (Boston)\n",
    "viz.price_analysis_plots(df_boston[neighborhood_cols], False)"
   ]
  },
  {
   "source": [
    "##### 3.2) Data Understanding\n",
    "\n",
    "While the head() method is useful, a more thorough understanding of all the available columns, datatypes, as well as profile of that data, is necessary before moving on to preparing the data for modeling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# And see the overall shape of each data frame, column names, and column data types\n",
    "DataWrngl.print_column_types(df_list)"
   ]
  },
  {
   "source": [
    "##### 3.3) Data Preperation\n",
    "In this section, we will be using our above observations to guide both the initial cleansing of our data, as well as initial transformations for feature creation.\n",
    "\n",
    "**Column Cleansing**\n",
    "***\n",
    "\n",
    "We'll start with the data cleansing process on the columns. This will largely involve removing:\n",
    "\n",
    "- [x]  100% empty\n",
    "- [x]  Columns with 100% of the same value\n",
    "- [x]  Database identifications\n",
    "- [x]  Non-overlapping columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "# Start with dropping columns in all data sets that are 100% na\n",
    "DataWrngl.drop_na_columns(df_list)\n",
    "\n",
    "# Define search words to drop common identification column suffixes\n",
    "search_words = ['_id','_url','_scraped']\n",
    "\n",
    "# Invoke function to drop id, url, and scraping columns\n",
    "DataWrngl.drop_ident_columns(df_list,search_words)\n",
    "\n",
    "# Invoke function to drop any columns containing rows with 100% the same data point\n",
    "DataWrngl.drop_same_columns(df_list)\n",
    "\n",
    "# Invoke function to drop non-overlapping columns between two datasets    \n",
    "DataWrngl.drop_nonoverlap_columns(df_list[0],df_list[1])\n",
    "\n",
    "# Check to make sure all columns are overlapping between dataframes\n",
    "sum((df_boston.columns == df_seattle.columns)) / ((len(df_boston.columns) + len(df_seattle.columns)) / 2 ) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row & Data Cleansing**\n",
    "***\n",
    "Next, we'll move into basic *row cleansing* for items we know will break a linear regression model. This includes:\n",
    "\n",
    "- [x]  True/False - transform to a true boolean rather than text \"t\" or \"f\".\n",
    "- [x]  Substantial Text - must be transformed into a text analytics feature.\n",
    "- [ ]  Date Columns - drop or transform into a duration if business questions warrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Invoke function to change 't' or 'f' to true Boolean values\n",
    "DataWrngl.cleanse_data_tf_to_boolean(df_list)\n",
    "\n",
    "#  Define text columns which require processing for text analytics\n",
    "text_columns = ['name','summary','space','description','neighborhood_overview', 'notes']\n",
    "\n",
    "# Invoke tokenization as add columns in both dataframes\n",
    "TxtWrngl.tokenize_text_columns(df_list,text_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['tokenized',\n",
       " 'description',\n",
       " 'name',\n",
       " 'neighborhood',\n",
       " 'note',\n",
       " 'overview',\n",
       " 'space',\n",
       " 'summary']"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "\n",
    "# Leverage pre-tokenized text columns to create bag of words dataframe\n",
    "tokenized_text_columns = [column + '_tokenized' for column in text_columns]\n",
    "\n",
    "# define finding top vocab\n",
    "def get_top_vocabulary(dataframe_list,column_list):\n",
    "\n",
    "    tokenized_df = pd.DataFrame()\n",
    "\n",
    "    for dataframe in dataframe_list:\n",
    "        tokenized_df = tokenized_df.append(dataframe[column_list])\n",
    "\n",
    "    # initialize count vectorizer object\n",
    "    vectorizer = CountVectorizer(tokenizer=TxtWrngl.tokenize)\n",
    "\n",
    "    # get counts of each token (word) in text data\n",
    "    top_vocab_vector = vectorizer.fit_transform(tokenized_df)\n",
    "\n",
    "    # Create dataframe to hold vocabulary and count of each item\n",
    "    top_vocab_df = pd.DataFrame((count,word) for word, count in zip(sum(top_vocab_vector.toarray()).tolist(),vectorizer.get_feature_names()))\n",
    "\n",
    "    # Assign column names to new dataframe\n",
    "    top_vocab_df.columns = ['Word', 'Count']\n",
    "\n",
    "    # Convert the top 20 words to a list for feature extraction\n",
    "    top_vocab = top_vocab_df.nlargest(20,'Count')['Word'].tolist()\n",
    "\n",
    "    return top_vocab\n",
    "\n",
    "result = get_top_vocabulary(df_list,tokenized_text_columns)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4) Data Modeling\n",
    "\n",
    "[Outline placeholder for data modeling section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5) Results Evaluation\n",
    "\n",
    "[Outline placeholder for evaluation of model and possible re-modeling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6) Deployment\n",
    "\n",
    "[Outline placeholder for what deployment steps would be]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Conclusion\n",
    "\n",
    "[Outline placeholder for final remarks]"
   ]
  }
 ]
}