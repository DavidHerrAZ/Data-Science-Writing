{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing A Data Science Blog Post\n",
    "## \n",
    "\n",
    "### By: David Herr\n",
    "### Dated: August 2nd, 2020\n",
    "\n",
    "A Jupyter notebook aimed to complete the Udacity Data Science Nanodegree Project 1. In the project, students are to use the CRISP-DM process to understand, prepare, model, and evaluate data to answer real-world business questions.\n",
    "\n",
    "First, we'll bring in the packages used to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\dlher\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\dlher\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\dlher\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# data import and manipulation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting and graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# text analysis, mining, and sentiment\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# custom modules\n",
    "import fns_DataVisualization as viz\n",
    "import fns_DataWrangling as wrngl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Pick A Dataset\n",
    "\n",
    "Although the traditional CRISP-DM path is to start with business questions, the Udacity Data Science program provided datasets to start with. Thus, we'll start with those datasets and read them in to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Boston dataset\n",
    "df_boston = pd.read_csv('./_data/listings_boston.csv')\n",
    "\n",
    "# Read in Seattle dataset\n",
    "df_seattle = pd.read_csv('./_data/listings_seattle.csv')\n",
    "\n",
    "# Place dataframes in list for bulk analysis\n",
    "df_list = [df_boston,df_seattle]\n"
   ]
  },
  {
   "source": [
    "### 2) Pose 3-5 Business Questions\n",
    "Leveraging this data, we are curious to find out if:\n",
    "\n",
    "    1. Bedroom or Bathroom count generally correlates to higher pricing?\n",
    "    2. Are there certain neighborhoods which command generally higher pricing?\n",
    "    3. Does containing a top N keyword in descriptions result in higher pricing?\n",
    "    4. Are ratings predictive of pricing?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3) Use CRISP_DM \n",
    "To answer these questions, we'll use what is known as the CRISP_DM process. This process is pictated below:\n",
    "\n",
    "![Image of CRISP_DM](_img\\CRISP_DM.png)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "##### 3.1) Business Understanding\n",
    "\n",
    "**Boston Bed/Bath Analysis**\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns to be used for price vs. bed/bath analysis\n",
    "bed_bath_cols = ['price','bedrooms','bathrooms','beds','bed_type']\n",
    "\n",
    "# call 2x2 price analysis plot for all bed/bath cols (Boston)\n",
    "viz.price_analysis_plots(df_boston[bed_bath_cols])"
   ]
  },
  {
   "source": [
    "**Boston Neighborhood Analysis**\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns to be used for price vs. neighbourhood analysis\n",
    "neighborhood_cols = ['price','neighbourhood_cleansed']\n",
    "\n",
    "# call 1x1 price analysis plot for neighborhoods cols (Boston)\n",
    "viz.price_analysis_plots(df_boston[neighborhood_cols], False)"
   ]
  },
  {
   "source": [
    "##### 3.2) Data Understanding\n",
    "\n",
    "While the head() method is useful, a more thorough understanding of all the available columns, datatypes, as well as profile of that data, is necessary before moving on to preparing the data for modeling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nDataframe Shape:(3585, 95)\nid:int64\nlisting_url:object\nscrape_id:int64\nlast_scraped:object\nname:object\nsummary:object\nspace:object\ndescription:object\nexperiences_offered:object\nneighborhood_overview:object\nnotes:object\ntransit:object\naccess:object\ninteraction:object\nhouse_rules:object\nthumbnail_url:object\nmedium_url:object\npicture_url:object\nxl_picture_url:object\nhost_id:int64\nhost_url:object\nhost_name:object\nhost_since:object\nhost_location:object\nhost_about:object\nhost_response_time:object\nhost_response_rate:object\nhost_acceptance_rate:object\nhost_is_superhost:object\nhost_thumbnail_url:object\nhost_picture_url:object\nhost_neighbourhood:object\nhost_listings_count:int64\nhost_total_listings_count:int64\nhost_verifications:object\nhost_has_profile_pic:object\nhost_identity_verified:object\nstreet:object\nneighbourhood:object\nneighbourhood_cleansed:object\nneighbourhood_group_cleansed:float64\ncity:object\nstate:object\nzipcode:object\nmarket:object\nsmart_location:object\ncountry_code:object\ncountry:object\nlatitude:float64\nlongitude:float64\nis_location_exact:object\nproperty_type:object\nroom_type:object\naccommodates:int64\nbathrooms:float64\nbedrooms:float64\nbeds:float64\nbed_type:object\namenities:object\nsquare_feet:float64\nprice:object\nweekly_price:object\nmonthly_price:object\nsecurity_deposit:object\ncleaning_fee:object\nguests_included:int64\nextra_people:object\nminimum_nights:int64\nmaximum_nights:int64\ncalendar_updated:object\nhas_availability:float64\navailability_30:int64\navailability_60:int64\navailability_90:int64\navailability_365:int64\ncalendar_last_scraped:object\nnumber_of_reviews:int64\nfirst_review:object\nlast_review:object\nreview_scores_rating:float64\nreview_scores_accuracy:float64\nreview_scores_cleanliness:float64\nreview_scores_checkin:float64\nreview_scores_communication:float64\nreview_scores_location:float64\nreview_scores_value:float64\nrequires_license:object\nlicense:float64\njurisdiction_names:float64\ninstant_bookable:object\ncancellation_policy:object\nrequire_guest_profile_picture:object\nrequire_guest_phone_verification:object\ncalculated_host_listings_count:int64\nreviews_per_month:float64\n\nDataframe Shape:(3818, 92)\nid:int64\nlisting_url:object\nscrape_id:int64\nlast_scraped:object\nname:object\nsummary:object\nspace:object\ndescription:object\nexperiences_offered:object\nneighborhood_overview:object\nnotes:object\ntransit:object\nthumbnail_url:object\nmedium_url:object\npicture_url:object\nxl_picture_url:object\nhost_id:int64\nhost_url:object\nhost_name:object\nhost_since:object\nhost_location:object\nhost_about:object\nhost_response_time:object\nhost_response_rate:object\nhost_acceptance_rate:object\nhost_is_superhost:object\nhost_thumbnail_url:object\nhost_picture_url:object\nhost_neighbourhood:object\nhost_listings_count:float64\nhost_total_listings_count:float64\nhost_verifications:object\nhost_has_profile_pic:object\nhost_identity_verified:object\nstreet:object\nneighbourhood:object\nneighbourhood_cleansed:object\nneighbourhood_group_cleansed:object\ncity:object\nstate:object\nzipcode:object\nmarket:object\nsmart_location:object\ncountry_code:object\ncountry:object\nlatitude:float64\nlongitude:float64\nis_location_exact:object\nproperty_type:object\nroom_type:object\naccommodates:int64\nbathrooms:float64\nbedrooms:float64\nbeds:float64\nbed_type:object\namenities:object\nsquare_feet:float64\nprice:object\nweekly_price:object\nmonthly_price:object\nsecurity_deposit:object\ncleaning_fee:object\nguests_included:int64\nextra_people:object\nminimum_nights:int64\nmaximum_nights:int64\ncalendar_updated:object\nhas_availability:object\navailability_30:int64\navailability_60:int64\navailability_90:int64\navailability_365:int64\ncalendar_last_scraped:object\nnumber_of_reviews:int64\nfirst_review:object\nlast_review:object\nreview_scores_rating:float64\nreview_scores_accuracy:float64\nreview_scores_cleanliness:float64\nreview_scores_checkin:float64\nreview_scores_communication:float64\nreview_scores_location:float64\nreview_scores_value:float64\nrequires_license:object\nlicense:float64\njurisdiction_names:object\ninstant_bookable:object\ncancellation_policy:object\nrequire_guest_profile_picture:object\nrequire_guest_phone_verification:object\ncalculated_host_listings_count:int64\nreviews_per_month:float64\n"
     ]
    }
   ],
   "source": [
    "# And see the overall shape of each data frame, column names, and column data types\n",
    "wrngl.print_column_types(df_list)"
   ]
  },
  {
   "source": [
    "##### 3.3) Data Preperation\n",
    "In this section, we will be using our above observations to guide both the initial cleansing of our data, as well as initial transformations for feature creation.\n",
    "\n",
    "**Column Cleansing**\n",
    "***\n",
    "\n",
    "We'll start with the data cleansing process on the columns. This will largely involve removing:\n",
    "\n",
    "- [x]  100% empty\n",
    "- [x]  Columns with 100% of the same value\n",
    "- [x]  Database identifications\n",
    "- [x]  Non-overlapping columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with dropping columns in all data sets that are 100% na\n",
    "wrngl.drop_na_columns(df_list)\n",
    "\n",
    "# Define search words to drop common identification column suffixes\n",
    "search_words = ['_id','_url','_scraped']\n",
    "\n",
    "# Invoke function to drop id, url, and scraping columns\n",
    "wrngl.drop_ident_columns(df_list,search_words)\n",
    "\n",
    "# Invoke function to drop any columns containing rows with 100% the same data point\n",
    "wrngl.drop_same_columns(df_list)\n",
    "\n",
    "# Invoke function to drop non-overlapping columns between two datasets    \n",
    "wrngl.drop_nonoverlap_columns(df_list[0],df_list[1])\n",
    "\n",
    "# Check to make sure all columns are overlapping between dataframes\n",
    "sum((df_boston.columns == df_seattle.columns)) / ((len(df_boston.columns) + len(df_seattle.columns)) / 2 ) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row & Data Cleansing**\n",
    "***\n",
    "Next, we'll move into basic *row cleansing* for items we know will break a linear regression model. This includes:\n",
    "\n",
    "- [x]  True/False - transform to a true boolean rather than text \"t\" or \"f\".\n",
    "- [x]  Substantial Text - must be transformed into a text analytics feature.\n",
    "- [ ]  Date Columns - drop or transform into a duration if business questions warrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Invoke function to change 't' or 'f' to true Boolean values\n",
    "wrngl.cleanse_data_tf_to_boolean(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Define text columns which require processing for text analytics\n",
    "text_cols = ['name','summary','space','description','neighborhood_overview', 'notes']\n",
    "\n",
    "# Begin text mining by defining stop words and lemmatizer dictionary\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to perform basic tokenization of text columns\n",
    "def tokenize(text):\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize andremove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Loop through dataframes and text cols to tokenize\n",
    "def tokenize_text_cols(dataframe_list):\n",
    "\n",
    "    for dataframe in dataframe_list:\n",
    "        for col in text_cols:\n",
    "            colname = col + '_tokenized'\n",
    "            dataframe[colname] = dataframe[col].apply(str).apply(tokenize)\n",
    "\n",
    "# Invoke tokenization as add columns in both dataframes\n",
    "tokenize_text_cols(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize count vectorizer object\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "\n",
    "# get counts of each token (word) in text data\n",
    "top_vocab = vectorizer.fit_transform(df_boston['summary'].apply(str))\n",
    "\n",
    "# Create dataframe to hold vocabulary and count of each item\n",
    "top_vocab_df = pd.DataFrame((count,word) for word, count in zip(sum(top_vocab.toarray()).tolist(),vectorizer.get_feature_names()))\n",
    "\n",
    "# Assign column names to new dataframe\n",
    "top_vocab_df.columns = ['Word', 'Count']\n",
    "\n",
    "# Sort descending to see top words\n",
    "top_vocab_df.sort_values('Count', ascending=False, inplace=True)\n",
    "\n",
    "# Preview results of dataframe\n",
    "top_vocab_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the top 20 words to a list for feature extraction\n",
    "top_vocab_df.nlargest(20,'Count')['Word'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4) Data Modeling\n",
    "\n",
    "[Outline placeholder for data modeling section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5) Results Evaluation\n",
    "\n",
    "[Outline placeholder for evaluation of model and possible re-modeling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6) Deployment\n",
    "\n",
    "[Outline placeholder for what deployment steps would be]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Conclusion\n",
    "\n",
    "[Outline placeholder for final remarks]"
   ]
  }
 ]
}