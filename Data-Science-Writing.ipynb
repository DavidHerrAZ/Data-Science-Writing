{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing A Data Science Blog Post\n",
    "## \n",
    "\n",
    "### By: David Herr\n",
    "### Dated: August 2nd, 2020\n",
    "\n",
    "A Jupyter notebook aimed to complete the Udacity Data Science Nanodegree Project 1. In the project, students are to use the CRISP-DM process to understand, prepare, model, and evaluate data to answer real-world business questions.\n",
    "\n",
    "First, we'll bring in the packages used to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import and manipulation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting and graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# text analysis, mining, and sentiment\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Pick A Dataset\n",
    "\n",
    "Although the traditional CRISP-DM path is to start with business questions, the Udacity Data Science program provided datasets to start with. Thus, we'll start with those datasets and read them in to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Boston dataset\n",
    "df_boston = pd.read_csv('./_data/listings_boston.csv')\n",
    "\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Seattle dataset\n",
    "df_seattle = pd.read_csv('./_data/listings_seattle.csv')\n",
    "\n",
    "df_seattle.head()"
   ]
  },
  {
   "source": [
    "### 2) Pose 3-5 Business Questions\n",
    "Leveraging this data, we are curious to find out if:\n",
    "\n",
    "    1. Bedroom or Bathroom count generally correlates to higher pricing?\n",
    "    2. Are there certain neighborhoods which command generally higher pricing?\n",
    "    3. Does containing a top N keyword in descriptions result in higher pricing?\n",
    "    4. Are ratings predictive of pricing?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3) Use CRISP_DM \n",
    "To answer these questions, we'll use what is known as the CRISP_DM process. This process is pictated below:\n",
    "\n",
    "![Image of CRISP_DM](_img\\CRISP_DM.png)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "##### 3.1) Business Understanding\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing must be cleaned as the column contains strings, floats, and ints.\n",
    "def clean_currency(x):\n",
    "    \"\"\" If the value is a string, then remove currency symbol and delimiters\n",
    "    otherwise, the value is numeric and can be converted\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        return(x.replace('$', '').replace(',', ''))\n",
    "    return(x)\n",
    "\n",
    "def price_analysis_plots(dataframe, is_vertical = True):\n",
    "\n",
    "    # Clean the pricing column for strings and integer variations\n",
    "    dataframe['price'] = dataframe['price'].apply(clean_currency).astype('float')\n",
    "\n",
    "    # Select all x variables in dataframe\n",
    "    x_variables =  dataframe.columns[dataframe.columns != 'price']\n",
    "\n",
    "    # Ensure all x variables are of string datatype\n",
    "    # Not totally robust, but provides simple compatibility with seaborn barplot() function\n",
    "    dataframe[x_variables] = dataframe[x_variables].astype('str')\n",
    "    \n",
    "    # How many categorical variables are there to plot?\n",
    "    x_count = len(x_variables)\n",
    "\n",
    "    # Set up a plot based on number of categorical variables\n",
    "    plt_rows = x_count // 2 + x_count % 2\n",
    "\n",
    "    if x_count > 1:\n",
    "        plt_cols = 2\n",
    "    else:\n",
    "        plt_cols = 1\n",
    "    \n",
    "    plt.figure(figsize = [5 * x_count, 16])\n",
    "\n",
    "    # Loop through variables to add to figure. \n",
    "    for index, variable in enumerate(x_variables):\n",
    "        plt.subplot(plt_rows, plt_cols,index + 1)\n",
    "        \n",
    "        # If horizontal requested, set-up fariable to flip chart horizontal\n",
    "        if is_vertical:\n",
    "            bar_orientation = 'v'\n",
    "            sns.barplot(x=variable,y='price',data=dataframe,orient=bar_orientation)\n",
    "        else:\n",
    "            bar_orientation = 'h'\n",
    "            sns.barplot(x='price',y=variable,data=dataframe,orient=bar_orientation)\n",
    "\n",
    "#define dataframes to plot against price\n",
    "boston_bed_bath_bar = df_boston[['price','bedrooms','bathrooms','beds','bed_type']]\n",
    "\n",
    "# Invoke function\n",
    "price_analysis_plots(boston_bed_bath_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up price for neighborhoods\n",
    "neighborhood_bar = df_boston[['price','neighbourhood_cleansed']]\n",
    "\n",
    "price_analysis_plots(neighborhood_bar, False)"
   ]
  },
  {
   "source": [
    "##### 3.2) Data Understanding\n",
    "\n",
    "While the head() method is useful, a more thorough understanding of all the available columns, datatypes, as well as profile of that data, is necessary before moving on to preparing the data for modeling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To be able to understand both datesets quickly, let's put them into a list\n",
    "df_list = [df_boston,df_seattle]\n",
    "\n",
    "# And see the overall shape of each data frame, column names, and column data types\n",
    "for dataframe in df_list:\n",
    "    print(\"Dataframe Shape:{}\".format(dataframe.shape))\n",
    "    for col in dataframe:\n",
    "        print(\"{}:{}\".format(col,dataframe[col].dtypes))"
   ]
  },
  {
   "source": [
    "##### 3.3) Data Preperation\n",
    "In this section, we will be using our above observations to guide both the initial cleansing of our data, as well as initial transformations for feature creation.\n",
    "\n",
    "**Column Cleansing**\n",
    "***\n",
    "\n",
    "We'll start with the data cleansing process on the columns. This will largely involve removing:\n",
    "\n",
    "- [x]  100% empty\n",
    "- [x]  Columns with 100% of the same value\n",
    "- [x]  Database identifications\n",
    "- [x]  Non-overlapping columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with dropping columns in all data sets that are 100% na\n",
    "for dataframe in df_list:\n",
    "    dataframe.dropna(axis = 1, how = 'all',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the unnecssary id, url, and scraping columns\n",
    "def drop_ident_columns(dataframe_list,word_list):\n",
    "    for dataframe in dataframe_list:\n",
    "        for word in word_list:\n",
    "            drop_columns = [col for col in dataframe.columns if col.endswith(word)]\n",
    "            dataframe.drop(drop_columns, axis = 1,inplace = True)\n",
    "\n",
    "# Define search words to drop common identification column suffixes\n",
    "search_words = ['_id','_url','_scraped']\n",
    "\n",
    "# Invoke function to drop id, url, and scraping columns\n",
    "drop_ident_columns(df_list,search_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove columns where all values are the same (equivalant of 100% empty)\n",
    "def drop_same_columns(dataframe_list):\n",
    "    for dataframe in dataframe_list:\n",
    "        nunique = dataframe.apply(pd.Series.nunique)\n",
    "        drop_columns = nunique[nunique ==1].index\n",
    "        dataframe.drop(drop_columns, axis = 1,inplace = True)\n",
    "\n",
    "# Invoke function to drop any columns containing rows with 100% the same data point\n",
    "drop_same_columns(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop columns which do not overlap between the two datasets\n",
    "def drop_nonoverlap_columns(df1,df2):\n",
    "    # Find overlapping/instersecting columns between the two datasets\n",
    "    intersecting_columns = set(df1.columns) & set(df2.columns)\n",
    "\n",
    "    # Assign each dataset it's own drop columns based off the complete list of intersections\n",
    "    for dataframe in df1, df2:\n",
    "        drop_columns = set(dataframe.columns) - intersecting_columns\n",
    "        dataframe.drop(drop_columns, axis = 1, inplace = True)\n",
    "\n",
    "# Invoke function to drop non-overlapping columns between two datasets    \n",
    "drop_nonoverlap_columns(df_boston,df_seattle)\n",
    "\n",
    "# Check to make sure all columns are overlapping between dataframes\n",
    "sum((df_boston.columns == df_seattle.columns)) / ((len(df_boston.columns) + len(df_seattle.columns)) / 2 ) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row & Data Cleansing**\n",
    "***\n",
    "Next, we'll move into basic *row cleansing* for items we know will break a linear regression model. This includes:\n",
    "\n",
    "- [x]  True/False - transform to a true boolean rather than text \"t\" or \"f\".\n",
    "- [x]  Substantial Text - must be transformed into a text analytics feature.\n",
    "- [ ]  Date Columns - drop or transform into a duration if business questions warrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to clean t/f columns to true Booleans\n",
    "def cleanse_data_tf_to_boolean(dataframe_list):\n",
    "\n",
    "    for dataframe in dataframe_list:\n",
    "\n",
    "        column_list = []\n",
    "\n",
    "        for col in dataframe.columns:\n",
    "            if sum(dataframe[col].isin(['t','f'])) == len(dataframe[col]):\n",
    "                column_list.append(col)\n",
    "\n",
    "        for col in column_list:\n",
    "            dataframe.loc[(dataframe[col] == 't'),col] = True\n",
    "            dataframe.loc[(dataframe[col] == 'f'),col] = False\n",
    "\n",
    "# Invoke function to change 't' or 'f' to true Boolean values\n",
    "cleanse_data_tf_to_boolean(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Define text columns which require processing for text analytics\n",
    "text_cols = ['name','summary','space','description','neighborhood_overview', 'notes']\n",
    "\n",
    "# Begin text mining by defining stop words and lemmatizer dictionary\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to perform basic tokenization of text columns\n",
    "def tokenize(text):\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize andremove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Loop through dataframes and text cols to tokenize\n",
    "def tokenize_text_cols(dataframe_list):\n",
    "\n",
    "    for dataframe in dataframe_list:\n",
    "        for col in text_cols:\n",
    "            colname = col + '_tokenized'\n",
    "            dataframe[colname] = dataframe[col].apply(str).apply(tokenize)\n",
    "\n",
    "# Invoke tokenization as add columns in both dataframes\n",
    "tokenize_text_cols(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             Word  Count\n",
       "2420      infront      1\n",
       "1791      expands      1\n",
       "3785       saigon      1\n",
       "3784       safety      1\n",
       "1799  explanation      1\n",
       "...           ...    ...\n",
       "4665         walk   1171\n",
       "740       bedroom   1186\n",
       "3743         room   1242\n",
       "532     apartment   1329\n",
       "857        boston   2446\n",
       "\n",
       "[4842 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2420</th>\n      <td>infront</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1791</th>\n      <td>expands</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3785</th>\n      <td>saigon</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3784</th>\n      <td>safety</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1799</th>\n      <td>explanation</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4665</th>\n      <td>walk</td>\n      <td>1171</td>\n    </tr>\n    <tr>\n      <th>740</th>\n      <td>bedroom</td>\n      <td>1186</td>\n    </tr>\n    <tr>\n      <th>3743</th>\n      <td>room</td>\n      <td>1242</td>\n    </tr>\n    <tr>\n      <th>532</th>\n      <td>apartment</td>\n      <td>1329</td>\n    </tr>\n    <tr>\n      <th>857</th>\n      <td>boston</td>\n      <td>2446</td>\n    </tr>\n  </tbody>\n</table>\n<p>4842 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "# initialize count vectorizer object\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "\n",
    "# get counts of each token (word) in text data\n",
    "top_vocab = vectorizer.fit_transform(df_boston['summary'].apply(str))\n",
    "\n",
    "# Create dataframe to hold vocabulary and count of each item\n",
    "top_vocab_df = pd.DataFrame((count,word) for word, count in zip(sum(top_vocab.toarray()).tolist(),vectorizer.get_feature_names()))\n",
    "\n",
    "# Assign column names to new dataframe\n",
    "top_vocab_df.columns = ['Word', 'Count']\n",
    "\n",
    "# Sort descending to see top words\n",
    "top_vocab_df.sort_values('Count', ascending=False, inplace=True)\n",
    "\n",
    "# Preview results of dataframe\n",
    "top_vocab_df.head()\n"
   ]
  },
  {
   "source": [
    "sum(top_vocab.toarray())"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 4,  4,  7, ...,  5,  1, 25], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4) Data Modeling\n",
    "\n",
    "[Outline placeholder for data modeling section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5) Results Evaluation\n",
    "\n",
    "[Outline placeholder for evaluation of model and possible re-modeling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6) Deployment\n",
    "\n",
    "[Outline placeholder for what deployment steps would be]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Conclusion\n",
    "\n",
    "[Outline placeholder for final remarks]"
   ]
  }
 ]
}